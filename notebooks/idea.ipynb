{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "Create Q&A chatbot that will retrieve information from a database of documents.\n",
    "\n",
    "Process:\n",
    "1. Load documents and split them into chunks\n",
    "1. Embedd chunks using OpenAI embeddings \n",
    "1. Store embedding vectors in Pinecone DB\n",
    "1. Query Top X vectors based on the (dot)similarity with the prompt\n",
    "1. Prompts are send the OpenAI API and GPT model provides an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wowczar/GitHub/leibniz-chat\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 41}},\n",
       " 'total_vector_count': 41}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import pandas as pd \n",
    "from uuid import uuid4\n",
    "from tqdm.auto import tqdm\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from src.vector_db import PineconeConnector\n",
    "\n",
    "ENV_VARS = dotenv_values()\n",
    "\n",
    "# Initialize pinecone API variables\n",
    "pinecone_api_key = ENV_VARS[\"PINECONE_API_KEY\"]\n",
    "pinecone_env = \"us-west4-gcp\"\n",
    "index_name = \"ec-decisions-test\"\n",
    "pine = PineconeConnector(index_name, pinecone_api_key, pinecone_env)\n",
    "pine.index.describe_index_stats()\n",
    "\n",
    "# Anitialize openai API key and embed model\n",
    "openai_api_key = ENV_VARS[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = openai_api_key\n",
    "embed_model = \"text-embedding-ada-002\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_document(doc_path):\n",
    "    \"\"\"\n",
    "    Create a list of paragraphs from a single document\n",
    "    \"\"\"\n",
    "    loader = TextLoader(doc_path, encoding='utf-8')\n",
    "    document = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )    \n",
    "    doc = text_splitter.split_documents(document)\n",
    "    return doc\n",
    "\n",
    "def create_dataset(documents:list):\n",
    "    \"\"\"\n",
    "    Create dataset from list of documents, including metadata\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for doc in documents:\n",
    "        chunks = load_split_document(doc)\n",
    "        dataset.extend({\n",
    "            \"id\": str(uuid4()),\n",
    "            \"text\": chunks[i].page_content,\n",
    "            \"source\": chunks[i].metadata[\"source\"],\n",
    "            \"chunk\": i\n",
    "        } for i in range(len(chunks)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"data/m8322_124_3.txt\", \"data/CELEX_52019XC1128(02)_EN_TXT.txt\", \"data/CELEX_32022R1925_EN_TXT.txt\"]\n",
    "dataset = create_dataset(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:20,  4.08s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# Split documents into batches of 100 chunks and embed\n",
    "dataset_split = (dataset[i:i+batch_size] for i in range(0, len(dataset), batch_size))\n",
    "\n",
    "for batch in tqdm(dataset_split):\n",
    "    texts = [i[\"text\"] for i in batch]\n",
    "    ids = [i[\"id\"] for i in batch]\n",
    "    meta = batch\n",
    "    result = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "    embeds = [record['embedding'] for record in result['data']]\n",
    "    # upsert to Pinecone\n",
    "    pine.index.upsert(vectors=zip(ids, embeds, meta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query):\n",
    "    # Embed query\n",
    "    r = openai.Embedding.create(input=query, engine=embed_model)\n",
    "    embeds = [record['embedding'] for record in r['data']]\n",
    "    return embeds\n",
    "\n",
    "def find_contexts(embeds):\n",
    "    pine_search = pine.index.query(embeds, top_k=5, include_metadata=True)\n",
    "    # Return context and source\n",
    "    contexts = [f\"{i['metadata']['text']} (source: {i['metadata']['text']})\" for i in pine_search[\"matches\"]]\n",
    "    return contexts\n",
    "\n",
    "def primer():\n",
    "    primer = f\"\"\"\n",
    "        You are Q&A bot who is an economic expert. A highly intelligent system that answers\n",
    "        user questions based on the information provided by the user above each question. \n",
    "        If the information can not be found in the information provided by the user you truthfully say \"I don't know\". \n",
    "        Don't mention that you answer based on the information provided but do mention the source.\n",
    "    \"\"\"\n",
    "    return primer\n",
    "\n",
    "def ask_leibniz(query):\n",
    "    embeds = embed_query(query)\n",
    "    contexts = find_contexts(embeds)\n",
    "    primer = primer()\n",
    "\n",
    "    answer = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": primer},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\" + query}\n",
    "        ]\n",
    "    )\n",
    "    return answer['choices'][0]['message']['content']\n",
    "\n",
    "def gradio_demo():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ## Ask LEIBNIZ\n",
    "            Hello! I am a Q&A chatbot that can help you retrieve information from a database of EC's documents.\n",
    "            I don't know much yet, but you can ask me about:\n",
    "            * [Summary of Commission Decision: Case AT.40099 — Google Android](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52019XC1128(02)&from=EN)\n",
    "            * [Case M.8322 – HEINEKEN UK / PUNCH TAVERNS SECURITISATION](https://ec.europa.eu/competition/mergers/cases/decisions/m8322_124_3.pdf)\n",
    "            * [Digital Markets Act](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32022R1925&from=EN)\n",
    "            \"\"\" \n",
    "        )\n",
    "            \n",
    "        with gr.Tab(\"Chat\"):\n",
    "            inputs = gr.Textbox(lines=2, placeholder=\"Your question here...\", label=\"Question\")\n",
    "            outputs =gr.Textbox(label=\"Answer\",placeholder=\"I am quite new so I don't know everything.\")\n",
    "            text_button = gr.Button(\"Submit\")\n",
    "        with gr.Tab(\"How do I work?\"):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                > Here is how I work:\n",
    "                First, I load selected European Commissions' documents and split them into smaller chunks. Then, I use OpenAI embeddings (*ada-002*) to convert each chunk into a numerical representation, which I store in Pinecone, a vector database.\n",
    "                When you ask me a question, I use the prompt to search for the most similar chunks in the Pinecone database. This helps me quickly find relevant information from the documents.\n",
    "                Next, I send the prompt to the OpenAI API, which uses a LLM (*GPT-3.5-turbo*) to provide an answer based on the most relevant chunks retrieved from Pinecone. The answer is then sent back to you as a response to your question.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        text_button.click(fn=ask_leibniz, inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # demo.launch(auth=(\"gottfried\", \"leibniz\"), share=True)\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7918\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7918/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradio_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leibniz-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
